{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tic Tac Toe Game\n",
    "Here the game definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tic_Tac_Toe:\n",
    "    \n",
    "    board: np.ndarray\n",
    "    actual_player: int\n",
    "    state: int\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.actual_player = 0\n",
    "        self.state = -1\n",
    "        \n",
    "    def possible_moves(self):\n",
    "        return np.argwhere(self.board == -1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.state = -1\n",
    "        self.actual_player = 0\n",
    "    \n",
    "    def move(self, position, player):\n",
    "        if player != self.actual_player:\n",
    "            raise ValueError(f\"Wrong player making the move\")\n",
    "        self.actual_player = 1-player\n",
    "        self.board[position] = player\n",
    "        self.state = self.check_winner()\n",
    "        if self.state == player:\n",
    "            self.state = player\n",
    "        if len(np.argwhere(self.board == -1)) == 0:\n",
    "            self.state = 2\n",
    "        return 1, self.state\n",
    "        \n",
    "    def check_winner(self) -> int:\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != -1:\n",
    "                return self.board[i][0]\n",
    "            \n",
    "            if self.board[0][i] == self.board[1][i] == self.board[2][i] != -1:\n",
    "                return self.board[0][i]\n",
    "\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != -1:\n",
    "            return self.board[0][0]\n",
    "\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != -1:\n",
    "            return self.board[0][2]\n",
    "\n",
    "        return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_number(number):\n",
    "        if number == -1:\n",
    "            return \"-\"\n",
    "        elif number == 0:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return \"X\"\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(3): \n",
    "            print(f\"{self.convert_number(self.board[i][0])}  {self.convert_number(self.board[i][1])}  {self.convert_number(self.board[i][2])}\")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def print_state(self):\n",
    "        if self.state == 2:\n",
    "            print(\"Draw\")\n",
    "        elif self.state == 0:\n",
    "            print(\"Player 0 wins\")\n",
    "        elif self.state == 1:\n",
    "            print(\"Player 1 wins\")\n",
    "        else:   \n",
    "            print(\"Play in progress\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:50:45.149970Z",
     "start_time": "2023-12-22T12:50:45.118777Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Agent\n",
    "A simple random agent that makes a random move considering the possible ones"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "    \n",
    "    player_number: int\n",
    "\n",
    "    def __init__(self, player_number: int):\n",
    "        self.player_number = player_number\n",
    "    \n",
    "    def move(self, state, possible_moves):\n",
    "        return tuple(choice(possible_moves))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:50:45.151857Z",
     "start_time": "2023-12-22T12:50:45.146243Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q Agent\n",
    "The agent use a q learning approach\n",
    "It uses some utility function to convert the states of the game in a string to memorize in the q table dictionary\n",
    "It does the same for the action to convert them from 2 dimensional to 1 dimensional ((2, 1) -> 8)\n",
    "It can use 3 different strategies to performa a move\n",
    "- An epsilon greedy policy that use the exploration rate to decide if take a random action or to choose the best action based on the actual data in\n",
    "- A UCB policy that computes a value for each action based on the number of times the action were used on the total number of action performed, and then take the max values\n",
    "- A Boltzmann or softmax policy that computes a softmax vector representing a probability distribution of the q values for the actual state and perform a choice based on the probabilities\n",
    "For every move at training time, it uses a random variable generation to choose if take a random move or to choose the best action based on the actual data in the q table\n",
    "After each episode, it adopts an exponential decrementation of the exploration rate\n",
    "The update in the Q table is based on this formula:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1 - \\alpha) * Q(s_t, a_t) + \\alpha * ( R_{t+1} + \\gamma * ( - \\max_a Q(s_{t+1}, a) ) )\n",
    "$$\n",
    "The minus sign in the above formula is used because the next state is the opponent state\n",
    "Note: I take this idea from [Davide Vitabile](https://github.com/Vitabile/Computational-Intelligence/tree/main)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import base64\n",
    "from typing import Dict, Tuple\n",
    "from random import random\n",
    "\n",
    "REWARD_WIN = 10\n",
    "REWARD_DRAW = 0\n",
    "REWARD_LOSE = -10\n",
    "\n",
    "\n",
    "class Q_Agent:\n",
    "    player_number: int\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    exploration_rate: float\n",
    "    min_exploration_rate: float\n",
    "    exploration_decay: float\n",
    "    q_table: Dict[Tuple, float]\n",
    "    exploration_strategy: int\n",
    "    \n",
    "    \n",
    "    def __init__(self, learning_rate: float, discount_rate: float, exploration_rate: float, min_exploration_rate: float, exploration_decay: float, opponent, exploration_strategy: int):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}\n",
    "        self.opponent = opponent\n",
    "        self.exploration_strategy = exploration_strategy\n",
    "    \n",
    "    def convert_state(self, state):\n",
    "        return \"\".join(str(_) for _ in state.flatten())\n",
    "    \n",
    "    def convert_action(self, action):\n",
    "        return action[0]*3+action[1]\n",
    "        \n",
    "    def move(self, state, possible_moves):\n",
    "        converted_state = self.convert_state(state)\n",
    "        if converted_state not in self.q_table:\n",
    "                self.q_table[converted_state] = np.zeros((9,))\n",
    "        if self.exploration_strategy == 0:\n",
    "            if random() < self.exploration_rate:\n",
    "                return tuple(choice(possible_moves))\n",
    "            else:\n",
    "                possible_moves = [self.convert_action(action) for action in possible_moves] \n",
    "                possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "                max_value = max(possible_values)\n",
    "                best_moves = [action for action, value in zip(possible_moves, possible_values) if value == max_value]\n",
    "                move = choice(best_moves)\n",
    "                return move // 3, move % 3\n",
    "        elif self.exploration_strategy == 1:\n",
    "            if random() < self.exploration_rate:\n",
    "                return tuple(choice(possible_moves))\n",
    "            else:\n",
    "                possible_moves = [self.convert_action(action) for action in possible_moves] \n",
    "                ucb_values = [self.q_table[converted_state][action] + np.sqrt(2 * np.log(len(possible_moves)) /\n",
    "                                  max(1, np.sum(self.q_table[converted_state][action])))\n",
    "                                for action in possible_moves]\n",
    "                move = possible_moves[np.argmax(ucb_values)]\n",
    "                return move // 3, move % 3\n",
    "        else:\n",
    "            possible_moves = [self.convert_action(action) for action in possible_moves]\n",
    "            possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "            max_value = np.max(possible_values)\n",
    "            scaled_values = [val - max_value for val in possible_values]\n",
    "            exp_values = np.exp(np.array(scaled_values) / self.exploration_rate)\n",
    "            boltzmann_probs = exp_values / np.sum(exp_values)\n",
    "            chosen_action_index = np.random.choice(len(possible_moves), p=boltzmann_probs)\n",
    "            move = possible_moves[chosen_action_index]\n",
    "            return move // 3, move % 3\n",
    "        \n",
    "    \n",
    "    def get_game_reward(self, winner):\n",
    "        if winner == self.player_number:\n",
    "            return REWARD_WIN\n",
    "        elif winner == 2:\n",
    "            return REWARD_DRAW\n",
    "        else:\n",
    "            return REWARD_LOSE\n",
    "    \n",
    "    def update_q_table(self, prev_state, action, reward, next_state):\n",
    "        action = self.convert_action(action)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros((9,))\n",
    "        if prev_state not in self.q_table:\n",
    "            self.q_table[prev_state] = np.zeros((9,))\n",
    "        new_q_table_value = ((1-self.learning_rate) * self.q_table[prev_state][action] + \n",
    "                             self.learning_rate * (reward + self.discount_rate * (-np.max(self.q_table[next_state]))))\n",
    "        self.q_table[prev_state][action] = new_q_table_value\n",
    "        \n",
    "    def train(self, n_episodes, first_Player=True):\n",
    "        game = Tic_Tac_Toe()\n",
    "        self.player_number = 0 if first_Player else 1\n",
    "        self.opponent.player_number = 1-self.player_number \n",
    "        players = [self, self.opponent]\n",
    "        turn = 0 if first_Player else 1\n",
    "        for episode in range(n_episodes):\n",
    "            while game.state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if game.actual_player == self.player_number:\n",
    "                    action = players[turn].move(game.board, possible_moves)\n",
    "                    reward, game_state = game.move(action, players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[turn].move(game.board, possible_moves), players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                turn = 1-turn\n",
    "            turn = 0 if first_Player else 1\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            game.reset()\n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episode), self.min_exploration_rate, 1\n",
    "            )\n",
    "        print(self.exploration_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:40:01.187830Z",
     "start_time": "2023-12-22T13:40:01.178709Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here i trained 3 agents, each one with a different strategies to perform an action "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "outputs": [],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=0)\n",
    "agent_greedy.train(1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:52:40.521819Z",
     "start_time": "2023-12-22T12:50:45.164130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049787217729293086\n"
     ]
    }
   ],
   "source": [
    "agent_ucb = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(0),\n",
    "                  exploration_strategy=1)\n",
    "agent_ucb.train(1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:42:43.639036Z",
     "start_time": "2023-12-22T13:40:03.887856Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_boltzmann = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(0),\n",
    "                  exploration_strategy=2)\n",
    "agent_boltzmann.train(1000000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "outputs": [],
   "source": [
    "def match(a1, a2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    players = [a1, a2]\n",
    "    i = 1\n",
    "    while game.state == -1:\n",
    "        i = 1-i\n",
    "        action = players[i].move(game.board, game.possible_moves())\n",
    "        game.move(action, players[i].player_number)\n",
    "    return game.state\n",
    "\n",
    "def test_agent(agent, opponent, n_match, first_Player=True):\n",
    "    victories = 0\n",
    "    draws = 0\n",
    "    players = [agent, opponent]\n",
    "    turn = 0 if first_Player else 1\n",
    "    agent.player_number = 0 if first_Player else 1\n",
    "    opponent.player_number = 1 - agent.player_number\n",
    "    for _ in range(n_match):\n",
    "        result = match(players[turn], players[1-turn])\n",
    "        if (result == 0 and first_Player) or (result == 1 and not first_Player):\n",
    "            victories += 1\n",
    "        if result == 2:\n",
    "            draws += 1\n",
    "    return victories, draws"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:36:47.816316Z",
     "start_time": "2023-12-22T13:36:47.812966Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the victories the best agent is the ucb but not for the loss since it does fewer draws and more losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent greedy: wins: 76682, draws: 11291 over 100000\n",
      "Explored states: 5478\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 100000\n",
    "agent_greedy.exploration_rate = 0\n",
    "wins, draws = test_agent(agent_greedy, RandomAgent(1), N_MATCHES)\n",
    "print(f\"Agent greedy: wins: {wins}, draws: {draws} over {N_MATCHES}\")\n",
    "print(f\"Explored states: {len(agent_greedy.q_table)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:09:02.329020Z",
     "start_time": "2023-12-22T13:08:54.408567Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ucb: wins: 84157, draws: 4896 over 100000\n",
      "Explored states: 5478\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 100000\n",
    "agent_ucb.exploration_rate = 0\n",
    "wins, draws = test_agent(agent_ucb, RandomAgent(1), N_MATCHES)\n",
    "print(f\"Agent ucb: wins: {wins}, draws: {draws} over {N_MATCHES}\")\n",
    "print(f\"Explored states: {len(agent_ucb.q_table)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:52:26.422950Z",
     "start_time": "2023-12-22T13:52:12.164420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent boltzmann: wins: 75007, draws: 12793 over 100000\n",
      "Explored states: 5474\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 100000\n",
    "agent_boltzmann.exploration_rate = 0.0001\n",
    "wins, draws = test_agent(agent_boltzmann, RandomAgent(1), N_MATCHES)\n",
    "print(f\"Agent boltzmann: wins: {wins}, draws: {draws} over {N_MATCHES}\")\n",
    "print(f\"Explored states: {len(agent_boltzmann.q_table)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:39:02.401733Z",
     "start_time": "2023-12-22T13:38:50.035014Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def match_with_board(a1, a2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    players = [a1, a2]\n",
    "    i = 1\n",
    "    while game.state == -1:\n",
    "        i = 1-i\n",
    "        action = players[i].move(game.board, game.possible_moves())\n",
    "        game.move(action, players[i].player_number)\n",
    "        game.print_board()\n",
    "    return game.state"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
