{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tic Tac Toe Game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tic_Tac_Toe:\n",
    "    \n",
    "    board: np.ndarray\n",
    "    actual_player: int\n",
    "    state: int\n",
    "    \n",
    "    def __init__(self, first_player=0):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.actual_player = first_player\n",
    "        self.state = -1\n",
    "        \n",
    "    def possible_moves(self):\n",
    "        return np.argwhere(self.board == -1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.actual_player = 0\n",
    "        self.results = -1\n",
    "    \n",
    "    def move(self, position, player):\n",
    "        if position not in self.possible_moves() or self.board[position]!= -1 :\n",
    "            raise ValueError(f\"Position {position} not valid\")\n",
    "        if player != self.actual_player:\n",
    "            raise ValueError(f\"Wrong player making the move\")\n",
    "        self.board[position] = player\n",
    "        self.state = self.check_winner()\n",
    "        self.actual_player = 1-player\n",
    "        if self.state == player:\n",
    "            self.state = player\n",
    "        if len(np.argwhere(self.board == -1)) == 0:\n",
    "            self.state = 2\n",
    "        return 1, self.state\n",
    "        \n",
    "    def check_winner(self) -> int:\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != -1:\n",
    "                return self.board[i][0]\n",
    "            \n",
    "            if self.board[0][i] == self.board[1][i] == self.board[2][i] != -1:\n",
    "                return self.board[0][i]\n",
    "\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != -1:\n",
    "            return self.board[0][0]\n",
    "\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != -1:\n",
    "            return self.board[0][2]\n",
    "\n",
    "        return -1\n",
    "    \n",
    "    def get_board(self) -> np.ndarray:\n",
    "        return self.board\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_number(number):\n",
    "        if number == -1:\n",
    "            return \"-\"\n",
    "        elif number == 0:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return \"X\"\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(3): \n",
    "            print(f\"{self.convert_number(self.board[i][0])}  {self.convert_number(self.board[i][1])}  {self.convert_number(self.board[i][2])}\")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def print_state(self):\n",
    "        if self.state == 2:\n",
    "            print(\"Draw\")\n",
    "        elif self.state == 0:\n",
    "            print(\"Player 0 wins\")\n",
    "        elif self.state == 1:\n",
    "            print(\"Player 1 wins\")\n",
    "        else:   \n",
    "            print(\"Play in progress\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:31.689954Z",
     "start_time": "2023-12-21T17:42:31.667279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "    \n",
    "    player_number: int\n",
    "\n",
    "    def __init__(self, player_number: int):\n",
    "        self.player_number = player_number\n",
    "    \n",
    "    def move(self, state, possible_moves):\n",
    "        return tuple(choice(possible_moves))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:31.691047Z",
     "start_time": "2023-12-21T17:42:31.673274Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "outputs": [],
   "source": [
    "def match(agent1, agent2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    agents = [agent1, agent2]\n",
    "    l = 0\n",
    "    while game.state == -1:\n",
    "        action = agents[l].move(game.board, game.possible_moves())\n",
    "        game.move(action, agents[l].player_number)\n",
    "        l = 1-l\n",
    "    return game.state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:31.700349Z",
     "start_time": "2023-12-21T17:42:31.693446Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    match(RandomAgent(0), RandomAgent(1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:31.702712Z",
     "start_time": "2023-12-21T17:42:31.696460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import base64\n",
    "from typing import Dict, Tuple\n",
    "from random import random\n",
    "\n",
    "REWARD_WIN = 10\n",
    "REWARD_DRAW = 0\n",
    "REWARD_LOSE = -10\n",
    "\n",
    "\n",
    "class Q_Agent:\n",
    "    player_number: int\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    exploration_rate: float\n",
    "    min_exploration_rate: float\n",
    "    exploration_decay: float\n",
    "    q_table: Dict[Tuple, float]\n",
    "    \n",
    "    def __init__(self, player_number: int, learning_rate: float, discount_rate: float, exploration_rate: float, min_exploration_rate: float, exploration_decay: float):\n",
    "        self.player_number = player_number\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}\n",
    "    \n",
    "    def convert_state(self, state):\n",
    "        return \"\".join(str(_) for _ in state.flatten())\n",
    "    \n",
    "    def convert_action(self, action):\n",
    "        return action[0]*3+action[1]\n",
    "        \n",
    "    def move(self, state, possible_moves):\n",
    "        converted_state = self.convert_state(state)\n",
    "        if converted_state not in self.q_table:\n",
    "                self.q_table[converted_state] = np.zeros((9,))\n",
    "        if random() < self.exploration_rate:\n",
    "            return tuple(choice(possible_moves))\n",
    "        else:\n",
    "            possible_moves = [self.convert_action(action) for action in possible_moves]  \n",
    "            possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "            max_value_index = np.argmax(possible_values)\n",
    "            move = possible_moves[max_value_index]\n",
    "            return move // 3, move % 3\n",
    "    \n",
    "    def get_game_reward(self, winner):\n",
    "        if winner == self.player_number:\n",
    "            return REWARD_WIN\n",
    "        elif winner == 2:\n",
    "            return REWARD_DRAW\n",
    "        else:\n",
    "            return REWARD_LOSE\n",
    "    \n",
    "    def update_q_table(self, prev_state, action, reward, next_state):\n",
    "        action = self.convert_action(action)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros((9,))\n",
    "        new_q_table_value = self.q_table[prev_state][action]*(1-self.learning_rate) + self.learning_rate * (reward + self.discount_rate * (-np.max(self.q_table[next_state])))\n",
    "        self.q_table[prev_state][action] = new_q_table_value\n",
    "        \n",
    "    def train(self, game, n_episodes):\n",
    "        players = [self, RandomAgent(1-self.player_number)]\n",
    "        l = 0\n",
    "        game_state = -1\n",
    "        for episodes in range(n_episodes):\n",
    "            while game_state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if l == self.player_number:\n",
    "                    action = players[l].move(game.board, possible_moves)\n",
    "                    reward, game_state = game.move(action, players[l].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[l].move(game.board, possible_moves), players[l].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                l = 1-l\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            game.reset()\n",
    "            game_state = -1\n",
    "            l = 0\n",
    "            self._exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episodes), self.min_exploration_rate, 1\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:31.710944Z",
     "start_time": "2023-12-21T17:42:31.707469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[775], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m q_agent \u001B[38;5;241m=\u001B[39m Q_Agent(player_number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \n\u001B[1;32m      2\u001B[0m                   learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, \n\u001B[1;32m      3\u001B[0m                   discount_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, \n\u001B[1;32m      4\u001B[0m                   exploration_rate\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m, \n\u001B[1;32m      5\u001B[0m                   min_exploration_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, \n\u001B[1;32m      6\u001B[0m                   exploration_decay\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3e-6\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mq_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTic_Tac_Toe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m500000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(q_agent\u001B[38;5;241m.\u001B[39mexploration_rate)\n",
      "Cell \u001B[0;32mIn[774], line 87\u001B[0m, in \u001B[0;36mQ_Agent.train\u001B[0;34m(self, game, n_episodes)\u001B[0m\n\u001B[1;32m     84\u001B[0m game_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     85\u001B[0m l \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exploration_rate \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(\n\u001B[0;32m---> 87\u001B[0m     \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexploration_decay\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_exploration_rate, \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     88\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "q_agent = Q_Agent(player_number=0, \n",
    "                  learning_rate=0.1, \n",
    "                  discount_rate=0.9, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.1, \n",
    "                  exploration_decay= 3e-6)\n",
    "q_agent.train(Tic_Tac_Toe(), 500000)\n",
    "print(q_agent.exploration_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:34.411650Z",
     "start_time": "2023-12-21T17:42:31.711663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"explored states: {len(q_agent.q_table)}\")\n",
    "\n",
    "rand1 = RandomAgent(1)\n",
    "q_agent.exploration_rate = 0.0\n",
    "wins = 0\n",
    "draw = 0\n",
    "for _ in range(10000):\n",
    "    result = match(q_agent, rand1)\n",
    "    if result == 0:\n",
    "        wins += 1\n",
    "    if result == 2:\n",
    "        draw += 1\n",
    "print(wins)\n",
    "print(draw)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:34.412735Z",
     "start_time": "2023-12-21T17:42:34.412192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:42:34.417678Z",
     "start_time": "2023-12-21T17:42:34.413959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T17:42:34.414717Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
