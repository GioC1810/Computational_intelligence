{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tic Tac Toe Game\n",
    "Here the game definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tic_Tac_Toe:\n",
    "    \n",
    "    board: np.ndarray\n",
    "    actual_player: int\n",
    "    state: int\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.actual_player = 0\n",
    "        self.state = -1\n",
    "        \n",
    "    def possible_moves(self):\n",
    "        return np.argwhere(self.board == -1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.state = -1\n",
    "        self.actual_player = 0\n",
    "    \n",
    "    def move(self, position, player):\n",
    "        if player != self.actual_player:\n",
    "            raise ValueError(f\"Wrong player making the move\")\n",
    "        self.actual_player = 1-player\n",
    "        self.board[position] = player\n",
    "        self.state = self.check_winner()\n",
    "        if self.state == player:\n",
    "            self.state = player\n",
    "        if len(np.argwhere(self.board == -1)) == 0:\n",
    "            self.state = 2\n",
    "        return 1, self.state\n",
    "        \n",
    "    def check_winner(self) -> int:\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != -1:\n",
    "                return self.board[i][0]\n",
    "            \n",
    "            if self.board[0][i] == self.board[1][i] == self.board[2][i] != -1:\n",
    "                return self.board[0][i]\n",
    "\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != -1:\n",
    "            return self.board[0][0]\n",
    "\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != -1:\n",
    "            return self.board[0][2]\n",
    "\n",
    "        return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_number(number):\n",
    "        if number == -1:\n",
    "            return \"-\"\n",
    "        elif number == 0:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return \"X\"\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(3): \n",
    "            print(f\"{self.convert_number(self.board[i][0])}  {self.convert_number(self.board[i][1])}  {self.convert_number(self.board[i][2])}\")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def print_state(self):\n",
    "        if self.state == 2:\n",
    "            print(\"Draw\")\n",
    "        elif self.state == 0:\n",
    "            print(\"Player 0 wins\")\n",
    "        elif self.state == 1:\n",
    "            print(\"Player 1 wins\")\n",
    "        else:   \n",
    "            print(\"Play in progress\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T10:41:54.107530Z",
     "start_time": "2023-12-22T10:41:54.078681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Agent\n",
    "A simple random agent that makes a random move considering the possible ones"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "    \n",
    "    player_number: int\n",
    "\n",
    "    def __init__(self, player_number: int):\n",
    "        self.player_number = player_number\n",
    "    \n",
    "    def move(self, state, possible_moves):\n",
    "        return tuple(choice(possible_moves))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T10:41:54.108594Z",
     "start_time": "2023-12-22T10:41:54.091896Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q Agent\n",
    "The agent use a q learning approach\n",
    "It uses some utility function to convert the states of the game in a string to memorize in the q table dictionary\n",
    "It does the same for the action to convert them from 2 dimensional to 1 dimensional ((2, 1) -> 8)\n",
    "It uses an exploration rate variable to apply a greedy policy\n",
    "For every move at training time, it uses a random variable generation to choose if take a random move or to choose the best action based on the actual data in the q table\n",
    "After each episode, it adopts an exponential decrementation of the exploration rate\n",
    "The update in the Q table is based on this formula:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1 - \\alpha) * Q(s_t, a_t) + \\alpha * ( R_{t+1} + \\gamma * ( - \\max_a Q(s_{t+1}, a) ) )\n",
    "$$\n",
    "The minus sign in the above formula is used because the next state is the opponent state\n",
    "Note: I take this idea from [Davide Vitabile](https://github.com/Vitabile/Computational-Intelligence/tree/main)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import base64\n",
    "from typing import Dict, Tuple\n",
    "from random import random\n",
    "\n",
    "REWARD_WIN = 10\n",
    "REWARD_DRAW = 0\n",
    "REWARD_LOSE = -10\n",
    "\n",
    "\n",
    "class Q_Agent:\n",
    "    player_number: int\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    exploration_rate: float\n",
    "    min_exploration_rate: float\n",
    "    exploration_decay: float\n",
    "    q_table: Dict[Tuple, float]\n",
    "    \n",
    "    \n",
    "    def __init__(self, learning_rate: float, discount_rate: float, exploration_rate: float, min_exploration_rate: float, exploration_decay: float, opponent):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}\n",
    "        self.opponent = opponent\n",
    "    \n",
    "    def convert_state(self, state):\n",
    "        return \"\".join(str(_) for _ in state.flatten())\n",
    "    \n",
    "    def convert_action(self, action):\n",
    "        return action[0]*3+action[1]\n",
    "        \n",
    "    def move(self, state, possible_moves):\n",
    "        converted_state = self.convert_state(state)\n",
    "        if converted_state not in self.q_table:\n",
    "                self.q_table[converted_state] = np.zeros((9,))\n",
    "        if random() < self.exploration_rate:\n",
    "            return tuple(choice(possible_moves))\n",
    "        else:\n",
    "            possible_moves = [self.convert_action(action) for action in possible_moves] \n",
    "            possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "            max_value = max(possible_values)\n",
    "            best_moves = [action for action, value in zip(possible_moves, possible_values) if value == max_value]\n",
    "            move = choice(best_moves)\n",
    "            return move // 3, move % 3\n",
    "    \n",
    "    def get_game_reward(self, winner):\n",
    "        if winner == self.player_number:\n",
    "            return REWARD_WIN\n",
    "        elif winner == 2:\n",
    "            return REWARD_DRAW\n",
    "        else:\n",
    "            return REWARD_LOSE\n",
    "    \n",
    "    def update_q_table(self, prev_state, action, reward, next_state):\n",
    "        action = self.convert_action(action)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros((9,))\n",
    "        if prev_state not in self.q_table:\n",
    "            self.q_table[prev_state] = np.zeros((9,))\n",
    "        new_q_table_value = ((1-self.learning_rate) * self.q_table[prev_state][action] + \n",
    "                             self.learning_rate * (reward + self.discount_rate * (-np.max(self.q_table[next_state]))))\n",
    "        self.q_table[prev_state][action] = new_q_table_value\n",
    "        \n",
    "    def train(self, n_episodes, first_Player=True):\n",
    "        game = Tic_Tac_Toe()\n",
    "        self.player_number = 0 if first_Player else 1\n",
    "        self.opponent.player_number = 1-self.player_number \n",
    "        players = [self, self.opponent]\n",
    "        turn = 0 if first_Player else 1\n",
    "        for episode in range(n_episodes):\n",
    "            while game.state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if game.actual_player == self.player_number:\n",
    "                    action = players[turn].move(game.board, possible_moves)\n",
    "                    reward, game_state = game.move(action, players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[turn].move(game.board, possible_moves), players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                turn = 1-turn\n",
    "            turn = 0 if first_Player else 1\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            game.reset()\n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episode), self.min_exploration_rate, 1\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T11:35:10.532955Z",
     "start_time": "2023-12-22T11:35:10.527725Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here i trained two agents, one which plays as first player, the other as seconds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "outputs": [],
   "source": [
    "agent_first_player = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1))\n",
    "agent_second_player = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(0))\n",
    "agent_first_player.train(1000000)\n",
    "agent_second_player.train(1000000, first_Player=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T11:41:08.393549Z",
     "start_time": "2023-12-22T11:35:42.040224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "outputs": [],
   "source": [
    "def match(a1, a2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    players = [a1, a2]\n",
    "    i = 1\n",
    "    while game.state == -1:\n",
    "        i = 1-i\n",
    "        action = players[i].move(game.board, game.possible_moves())\n",
    "        game.move(action, players[i].player_number)\n",
    "    return game.state\n",
    "\n",
    "def test_agent(agent, opponent, n_match, first_Player=True):\n",
    "    victories = 0\n",
    "    draws = 0\n",
    "    players = [agent, opponent]\n",
    "    agent.exploration_rate = 0.0\n",
    "    turn = 0 if first_Player else 1\n",
    "    agent.player_number = 0 if first_Player else 1\n",
    "    opponent.player_number = 1 - agent.player_number\n",
    "    for _ in range(n_match):\n",
    "        result = match(players[turn], players[1-turn])\n",
    "        if (result == 0 and first_Player) or (result == 1 and not first_Player):\n",
    "            victories += 1\n",
    "        if result == 2:\n",
    "            draws += 1\n",
    "    return victories, draws"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T11:59:45.609214Z",
     "start_time": "2023-12-22T11:59:45.383608Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first player agent perform pretty well but not perfect since it lost about 10-15% of the game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent as first player: wins: 7563, draws: 1241 over 10000\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_first_player.exploration_rate = 0\n",
    "wins, draws = test_agent(agent_first_player, RandomAgent(1), N_MATCHES)\n",
    "print(f\"Agent as first player: wins: {wins}, draws: {draws} over {N_MATCHES}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T11:59:48.452773Z",
     "start_time": "2023-12-22T11:59:47.190339Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second player agent perform bad since it wins about 45-50% of the games\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent as second player: wins: 5031, draws: 2085 over 10000\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_second_player.exploration_rate = 0\n",
    "wins, draws = test_agent(agent_second_player, RandomAgent(1), N_MATCHES, first_Player=False)\n",
    "print(f\"Agent as second player: wins: {wins}, draws: {draws} over {N_MATCHES}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:00:00.539131Z",
     "start_time": "2023-12-22T11:59:59.214176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -\n",
      "\n",
      "-  0  -\n",
      "\n",
      "-  -  -\n",
      "\n",
      "\n",
      "-  -  -\n",
      "\n",
      "-  0  -\n",
      "\n",
      "-  -  X\n",
      "\n",
      "\n",
      "0  -  -\n",
      "\n",
      "-  0  -\n",
      "\n",
      "-  -  X\n",
      "\n",
      "\n",
      "0  -  -\n",
      "\n",
      "-  0  X\n",
      "\n",
      "-  -  X\n",
      "\n",
      "\n",
      "0  0  -\n",
      "\n",
      "-  0  X\n",
      "\n",
      "-  -  X\n",
      "\n",
      "\n",
      "0  0  -\n",
      "\n",
      "-  0  X\n",
      "\n",
      "X  -  X\n",
      "\n",
      "\n",
      "0  0  0\n",
      "\n",
      "-  0  X\n",
      "\n",
      "X  -  X\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_with_board(a1, a2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    players = [a1, a2]\n",
    "    i = 1\n",
    "    while game.state == -1:\n",
    "        i = 1-i\n",
    "        action = players[i].move(game.board, game.possible_moves())\n",
    "        game.move(action, players[i].player_number)\n",
    "        game.print_board()\n",
    "    return game.state\n",
    "\n",
    "match_with_board(agent_first_player, RandomAgent(1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:01:06.765180Z",
     "start_time": "2023-12-22T12:01:06.761457Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent explored states: 5478\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agent explored states: {len(agent_first_player.q_table)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T11:55:26.865100Z",
     "start_time": "2023-12-22T11:55:26.842435Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
