{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tic Tac Toe Game\n",
    "Here the game definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tic_Tac_Toe:\n",
    "    \n",
    "    board: np.ndarray\n",
    "    actual_player: int\n",
    "    state: int\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.actual_player = 0\n",
    "        self.state = -1\n",
    "        \n",
    "    def possible_moves(self):\n",
    "        return np.argwhere(self.board == -1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.ones((3, 3), dtype=np.int8) * -1\n",
    "        self.state = -1\n",
    "        self.actual_player = 0\n",
    "    \n",
    "    def move(self, position, player):\n",
    "        if player != self.actual_player:\n",
    "            raise ValueError(f\"Wrong player making the move\")\n",
    "        reward = .1\n",
    "        self.actual_player = 1-player\n",
    "        self.board[position] = player\n",
    "        self.state = self.check_winner()\n",
    "        if len(np.argwhere(self.board == -1)) == 0:\n",
    "            self.state = 2\n",
    "        return reward, self.state\n",
    "        \n",
    "    def check_winner(self) -> int:\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != -1:\n",
    "                return self.board[i][0]\n",
    "            \n",
    "            if self.board[0][i] == self.board[1][i] == self.board[2][i] != -1:\n",
    "                return self.board[0][i]\n",
    "\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != -1:\n",
    "            return self.board[0][0]\n",
    "\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != -1:\n",
    "            return self.board[0][2]\n",
    "\n",
    "        return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_number(number):\n",
    "        if number == -1:\n",
    "            return \"-\"\n",
    "        elif number == 0:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return \"X\"\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(3): \n",
    "            print(f\"{self.convert_number(self.board[i][0])}  {self.convert_number(self.board[i][1])}  {self.convert_number(self.board[i][2])}\")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def print_state(self):\n",
    "        if self.state == 2:\n",
    "            print(\"Draw\")\n",
    "        elif self.state == 0:\n",
    "            print(\"Player 0 wins\")\n",
    "        elif self.state == 1:\n",
    "            print(\"Player 1 wins\")\n",
    "        else:   \n",
    "            print(\"Play in progress\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:06:22.570443Z",
     "start_time": "2024-02-07T16:06:21.864678Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Agent\n",
    "A simple random agent that makes a random move considering the possible ones"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "    \n",
    "    player_number: int\n",
    "\n",
    "    def __init__(self, player_number: int):\n",
    "        self.player_number = player_number\n",
    "    \n",
    "    def move(self, state, possible_moves):\n",
    "        return tuple(choice(possible_moves))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:06:22.581257Z",
     "start_time": "2024-02-07T16:06:22.572749Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q Agent\n",
    "The agent use a q learning approach\n",
    "It uses some utility function to convert the states of the game in a string to memorize in the q table dictionary\n",
    "It does the same for the action to convert them from 2 dimensional to 1 dimensional ((2, 1) -> 8)\n",
    "It can use 3 different strategies to performa a move\n",
    "- An epsilon greedy policy that use the exploration rate to decide if take a random action or to choose the best action based on the actual data in\n",
    "- A UCB policy that computes a value for each action based on the number of times the action were used on the total number of action performed, and then take the max values\n",
    "- A Boltzmann or softmax policy that computes a softmax vector representing a probability distribution of the q values for the actual state and perform a choice based on the probabilities\n",
    "For every move at training time, it uses a random variable generation to choose if take a random move or to choose the best action based on the actual data in the q table\n",
    "After each episode, it adopts an exponential decrementation of the exploration rate\n",
    "The update in the Q table is based on this formula:\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow (1 - \\alpha) * Q(s_t, a_t) + \\alpha * ( R_{t+1} + \\gamma * ( - \\max_a Q(s_{t+1}, a) ) )\n",
    "$$\n",
    "The minus sign in the above formula is used because the next state is the opponent state\n",
    "Note: I take this idea from [Davide Vitabile](https://github.com/Vitabile/Computational-Intelligence/tree/main)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import base64\n",
    "from typing import Dict, Tuple\n",
    "from random import random\n",
    "\n",
    "REWARD_WIN = 1\n",
    "REWARD_DRAW = 0\n",
    "REWARD_LOSE = -1\n",
    "\n",
    "\n",
    "class Q_Agent:\n",
    "    player_number: int\n",
    "    learning_rate: float\n",
    "    discount_rate: float\n",
    "    exploration_rate: float\n",
    "    min_exploration_rate: float\n",
    "    exploration_decay: float\n",
    "    q_table: Dict[Tuple, float]\n",
    "    exploration_strategy: int\n",
    "    \n",
    "    \n",
    "    def __init__(self, learning_rate: float, discount_rate: float, exploration_rate: float, min_exploration_rate: float, exploration_decay: float, opponent, exploration_strategy: int):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}\n",
    "        self.opponent = opponent\n",
    "        self.exploration_strategy = exploration_strategy\n",
    "    \n",
    "    def convert_state(self, state):\n",
    "        return \"\".join(str(_) for _ in state.flatten())\n",
    "    \n",
    "    def convert_action(self, action):\n",
    "        return action[0]*3+action[1]\n",
    "        \n",
    "    def move(self, state, possible_moves):\n",
    "        converted_state = self.convert_state(state)\n",
    "        if converted_state not in self.q_table:\n",
    "                self.q_table[converted_state] = np.zeros((9,))\n",
    "        if self.exploration_strategy == 0:\n",
    "            if random() < self.exploration_rate:\n",
    "                return tuple(choice(possible_moves))\n",
    "            else:\n",
    "                possible_moves = [self.convert_action(action) for action in possible_moves] \n",
    "                possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "                max_value = max(possible_values)\n",
    "                best_moves = [action for action, value in zip(possible_moves, possible_values) if value == max_value]\n",
    "                move = choice(best_moves)\n",
    "                return move // 3, move % 3    \n",
    "        elif self.exploration_strategy == 1:\n",
    "            if random() < self.exploration_rate:\n",
    "                return tuple(choice(possible_moves))\n",
    "            else:\n",
    "                possible_moves = [self.convert_action(action) for action in possible_moves] \n",
    "                ucb_values = [self.q_table[converted_state][action] + np.sqrt(2 * np.log(len(possible_moves)) /\n",
    "                                  max(1, np.sum(self.q_table[converted_state][action])))\n",
    "                                for action in possible_moves]\n",
    "                move = possible_moves[np.argmax(ucb_values)]\n",
    "                return move // 3, move % 3\n",
    "        else:\n",
    "            possible_moves = [self.convert_action(action) for action in possible_moves]\n",
    "            possible_values = [self.q_table[converted_state][action]  for action in possible_moves]\n",
    "            max_value = np.max(possible_values)\n",
    "            scaled_values = [val - max_value for val in possible_values]\n",
    "            exp_values = np.exp(np.array(scaled_values) / self.exploration_rate)\n",
    "            boltzmann_probs = exp_values / np.sum(exp_values)\n",
    "            chosen_action_index = np.random.choice(len(possible_moves), p=boltzmann_probs)\n",
    "            move = possible_moves[chosen_action_index]\n",
    "            return move // 3, move % 3\n",
    "        \n",
    "    def get_game_reward(self, winner):\n",
    "        if winner == self.player_number:\n",
    "            return REWARD_WIN\n",
    "        elif winner == 2:\n",
    "            return REWARD_DRAW\n",
    "        else:\n",
    "            return REWARD_LOSE\n",
    "    \n",
    "    def update_q_table(self, prev_state, action, reward, next_state):\n",
    "        action = self.convert_action(action)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros((9,))\n",
    "        if prev_state not in self.q_table:\n",
    "            self.q_table[prev_state] = np.zeros((9,))\n",
    "        self.q_table[prev_state][action] = ((1-self.learning_rate) * self.q_table[prev_state][action] + \n",
    "                                            self.learning_rate * (reward + self.discount_rate * (-np.max(self.q_table[next_state]))))\n",
    "        \n",
    "    def train(self, n_episodes, first_Player=True):\n",
    "        game = Tic_Tac_Toe()\n",
    "        self.player_number = 0 if first_Player else 1\n",
    "        self.opponent.player_number = 1-self.player_number \n",
    "        players = [self, self.opponent]\n",
    "        turn = 0 if first_Player else 1\n",
    "        for episode in range(n_episodes):\n",
    "            while game.state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if game.actual_player == self.player_number:\n",
    "                    action = players[turn].move(game.board, possible_moves)\n",
    "                    reward, game_state = game.move(action, players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[turn].move(game.board, possible_moves), players[turn].player_number)\n",
    "                turn = 1-turn\n",
    "            turn = 0 if first_Player else 1\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            game.reset()  \n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episode), self.min_exploration_rate, 1\n",
    "            )\n",
    "            \n",
    "    def train_backprop(self, n_episodes, first_Player=True):\n",
    "        game = Tic_Tac_Toe()\n",
    "        self.player_number = 0 if first_Player else 1\n",
    "        self.opponent.player_number = 1-self.player_number \n",
    "        players = [self, self.opponent]\n",
    "        turn = 0 if first_Player else 1\n",
    "        for episode in range(n_episodes):\n",
    "            states_action_traversed = []\n",
    "            while game.state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if game.actual_player == self.player_number:\n",
    "                    action = players[turn].move(game.board, possible_moves)\n",
    "                    states_action_traversed.append((actual_state, action))\n",
    "                    _, game_state = game.move(action, players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    #self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[turn].move(game.board, possible_moves), players[turn].player_number)\n",
    "                turn = 1-turn\n",
    "            turn = 0 if first_Player else 1\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            for state, action in states_action_traversed[::-1]:\n",
    "                self.update_q_table(state, action, game_reward, next_state)\n",
    "            game.reset()  \n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episode), self.min_exploration_rate, 1\n",
    "            )\n",
    "            \n",
    "    def train_backprop_incremental(self, n_episodes, first_Player=True):\n",
    "        game = Tic_Tac_Toe()\n",
    "        self.player_number = 0 if first_Player else 1\n",
    "        self.opponent.player_number = 1-self.player_number \n",
    "        players = [self, self.opponent]\n",
    "        turn = 0 if first_Player else 1\n",
    "        for episode in range(n_episodes):\n",
    "            states_action_traversed = []\n",
    "            while game.state == -1:\n",
    "                possible_moves = game.possible_moves()\n",
    "                actual_state = self.convert_state(game.board)\n",
    "                if game.actual_player == self.player_number:\n",
    "                    action = players[turn].move(game.board, possible_moves)\n",
    "                    states_action_traversed.append((actual_state, action))\n",
    "                    _, game_state = game.move(action, players[turn].player_number)\n",
    "                    next_state = self.convert_state(game.board)\n",
    "                    #self.update_q_table(actual_state, action, reward, next_state)\n",
    "                else:\n",
    "                    game.move(players[turn].move(game.board, possible_moves), players[turn].player_number)\n",
    "                turn = 1-turn\n",
    "            turn = 0 if first_Player else 1\n",
    "            game_reward = self.get_game_reward(game_state)\n",
    "            self.update_q_table(actual_state, action, game_reward, next_state)\n",
    "            reward_step_decrement = game_reward / len(states_action_traversed)\n",
    "            for state, action in states_action_traversed[::-1]:\n",
    "                self.update_q_table(state, action, game_reward, next_state)\n",
    "                game_reward -= reward_step_decrement if game_reward > 0 else -reward_step_decrement\n",
    "            game.reset()  \n",
    "            self.exploration_rate = np.clip(\n",
    "                np.exp(-self.exploration_decay * episode), self.min_exploration_rate, 1\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:06:25.524317Z",
     "start_time": "2024-02-07T16:06:25.517173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def match(a1, a2):\n",
    "    game = Tic_Tac_Toe()\n",
    "    players = [a1, a2]\n",
    "    i = 1\n",
    "    while game.state == -1:\n",
    "        i = 1-i\n",
    "        action = players[i].move(game.board, game.possible_moves())\n",
    "        game.move(action, players[i].player_number)\n",
    "    return game.state\n",
    "\n",
    "def test_agent(agent, opponent, n_match, first_Player=True):\n",
    "    victories = 0\n",
    "    draws = 0\n",
    "    players = [agent, opponent]\n",
    "    turn = 0 if first_Player else 1\n",
    "    agent.player_number = 0 if first_Player else 1\n",
    "    opponent.player_number = 1 - agent.player_number\n",
    "    for _ in range(n_match):\n",
    "        result = match(players[turn], players[1-turn])\n",
    "        if (result == 0 and first_Player) or (result == 1 and not first_Player):\n",
    "            victories += 1\n",
    "        if result == 2:\n",
    "            draws += 1\n",
    "    print(f\"Wins: {victories}, draws: {draws} over {N_MATCHES}\")\n",
    "    print(f\"Explored states: {len(agent.q_table)}\")\n",
    "    return victories, draws"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:06:27.049120Z",
     "start_time": "2024-02-07T16:06:27.045722Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here i trained 3 agents as first player, each one with a different strategies to perform an action "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration rate: 0.22313082953991437\n",
      "Wins: 7554, draws: 1307 over 10000\n",
      "Explored states: 5162\n"
     ]
    },
    {
     "data": {
      "text/plain": "(7554, 1307)"
     },
     "execution_count": 1228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=0)\n",
    "agent_greedy.train(500000)\n",
    "print(f\"Exploration rate: {agent_greedy.exploration_rate}\")\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T15:42:15.084051Z",
     "start_time": "2023-12-24T15:41:18.143074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 8383, draws: 547 over 10000\n",
      "Explored states: 5162\n"
     ]
    },
    {
     "data": {
      "text/plain": "(8383, 547)"
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_ucb = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(0),\n",
    "                  exploration_strategy=1)\n",
    "agent_ucb.train(2000000)\n",
    "N_MATCHES = 10000\n",
    "agent_ucb.exploration_rate = 0\n",
    "test_agent(agent_ucb, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T15:07:35.166418Z",
     "start_time": "2023-12-24T15:01:58.324519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "agent_boltzmann = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(0),\n",
    "                  exploration_strategy=2)\n",
    "agent_boltzmann.train(1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:09:18.495865Z",
     "start_time": "2024-02-07T16:06:32.074042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 7587, draws: 1242 over 10000\n",
      "Explored states: 5162\n"
     ]
    },
    {
     "data": {
      "text/plain": "(7587, 1242)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_boltzmann.exploration_rate = 0.00001\n",
    "test_agent(agent_boltzmann, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T16:10:31.180601Z",
     "start_time": "2024-02-07T16:10:29.671334Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the victories as first player the best agent is the ucb but not for the loss since it does fewer draws and more losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 7256, draws: 1468 over 10000\n",
      "Explored states: 5478\n"
     ]
    },
    {
     "data": {
      "text/plain": "(7256, 1468)"
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T14:07:21.731182Z",
     "start_time": "2023-12-24T14:07:20.834639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 8473, draws: 484 over 10000\n",
      "Explored states: 5478\n"
     ]
    },
    {
     "data": {
      "text/plain": "(8473, 484)"
     },
     "execution_count": 1146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_ucb.exploration_rate = 0\n",
    "test_agent(agent_ucb, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T10:51:23.336468Z",
     "start_time": "2023-12-23T10:51:21.841628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 7532, draws: 1253 over 10000\n",
      "Explored states: 5474\n"
     ]
    }
   ],
   "source": [
    "N_MATCHES = 10000\n",
    "agent_boltzmann.exploration_rate = 0.0001\n",
    "wins, draws = test_agent(agent_boltzmann, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T10:51:46.320213Z",
     "start_time": "2023-12-23T10:51:44.960902Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here i trained 3 agents as second player, each one with a different strategies to perform an action "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 5300, draws: 1976 over 10000\n",
      "Explored states: 5477\n"
     ]
    },
    {
     "data": {
      "text/plain": "(5300, 1976)"
     },
     "execution_count": 1199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy_2_player = Q_Agent(learning_rate=0.1,\n",
    "                       discount_rate=0.99,\n",
    "                       exploration_rate=1,\n",
    "                       min_exploration_rate=0.01,\n",
    "                       exploration_decay=3e-6,\n",
    "                       opponent=RandomAgent(1),\n",
    "                       exploration_strategy=0)\n",
    "agent_greedy_2_player.train(1000000, first_Player=False)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy_2_player.exploration_rate = 0\n",
    "test_agent(agent_greedy_2_player, RandomAgent(1), N_MATCHES, first_Player=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T13:34:18.631218Z",
     "start_time": "2023-12-24T13:32:19.405190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_ucb_2_player = Q_Agent(learning_rate=0.1,\n",
    "                    discount_rate=0.99,\n",
    "                    exploration_rate=1,\n",
    "                    min_exploration_rate=0.01,\n",
    "                    exploration_decay=3e-6,\n",
    "                    opponent=RandomAgent(0),\n",
    "                    exploration_strategy=1)\n",
    "agent_ucb_2_player.train(1000000,  first_Player=False)\n",
    "N_MATCHES = 10000\n",
    "agent_ucb_2_player.exploration_rate = 0\n",
    "test_agent(agent_ucb_2_player, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_boltzmann_2_player = Q_Agent(learning_rate=0.1,\n",
    "                          discount_rate=0.99,\n",
    "                          exploration_rate=1,\n",
    "                          min_exploration_rate=0.01,\n",
    "                          exploration_decay=3e-6,\n",
    "                          opponent=RandomAgent(0),\n",
    "                          exploration_strategy=2)\n",
    "agent_boltzmann_2_player.train(1000000, first_Player=False)\n",
    "N_MATCHES = 10000\n",
    "agent_boltzmann_2_player.exploration_rate = 0.00001\n",
    "test_agent(agent_boltzmann_2_player, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.1, \n",
    "                  discount_rate=0.99, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=0)\n",
    "agent_greedy.train(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 7407, draws: 1397 over 10000\n",
      "Explored states: 5478\n"
     ]
    },
    {
     "data": {
      "text/plain": "(7407, 1397)"
     },
     "execution_count": 1198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy.exploration_rate = 0\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T13:15:24.071868Z",
     "start_time": "2023-12-24T13:15:23.190703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9682, draws: 231 over 10000\n",
      "Explored states: 3925\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9682, 231)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=0)\n",
    "agent_greedy.train_backprop(500000)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T17:01:07.541675Z",
     "start_time": "2024-01-23T17:00:13.031935Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9283, draws: 621 over 10000\n",
      "Explored states: 3925\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9283, 621)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=1)\n",
    "agent_greedy.train_backprop(500000)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T17:03:43.659606Z",
     "start_time": "2024-01-23T17:02:30.633033Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9846, draws: 154 over 10000\n",
      "Explored states: 3925\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9846, 154)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=2)\n",
    "agent_greedy.train_backprop(1000000)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0.00001\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T17:09:32.148360Z",
     "start_time": "2024-01-23T17:08:08.643560Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9223, draws: 279 over 10000\n",
      "Explored states: 3991\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9223, 279)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=2)\n",
    "agent_greedy.train_backprop(1000000, first_Player=False)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0.00001\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES, first_Player=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T09:44:24.300042Z",
     "start_time": "2024-01-24T09:41:50.162729Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9955, draws: 45 over 10000\n",
      "Explored states: 3925\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9955, 45)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=2)\n",
    "agent_greedy.train_backprop_incremental(1000000)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0.00001\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T13:53:28.873158Z",
     "start_time": "2024-01-24T13:50:53.095779Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 9180, draws: 160 over 10000\n",
      "Explored states: 3991\n"
     ]
    },
    {
     "data": {
      "text/plain": "(9180, 160)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_greedy = Q_Agent(learning_rate=0.2, \n",
    "                  discount_rate=0.8, \n",
    "                  exploration_rate= 1, \n",
    "                  min_exploration_rate=0.01, \n",
    "                  exploration_decay= 3e-6,\n",
    "                  opponent=RandomAgent(1),\n",
    "                  exploration_strategy=2)\n",
    "agent_greedy.train_backprop_incremental(1000000, first_Player=False)\n",
    "N_MATCHES = 10000\n",
    "agent_greedy.exploration_rate = 0.00001\n",
    "test_agent(agent_greedy, RandomAgent(1), N_MATCHES, first_Player=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T13:56:44.615463Z",
     "start_time": "2024-01-24T13:54:13.026257Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
